{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "import cv2, os, random, importlib, numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "# import custom scripts \n",
    "import preprocess_images\n",
    "import get_bounding_box\n",
    "\n",
    "# reload and declare functions \n",
    "importlib.reload(preprocess_images)\n",
    "importlib.reload(get_bounding_box)\n",
    "\n",
    "from preprocess_images import preprocess_grayscale, preprocess_rgb, preprocess_rgbd\n",
    "from get_bounding_box import get_bounding_box, get_bounding_box_circumscribed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocess_images file as 3 functions, each cooresponding to the file type. All 3 functions are void functions that return the variables listed below. There is also an \"og\" (original) variable wihtin the python script that is not returned that can be used to find the unaltered image.\n",
    "\n",
    "Ex: \n",
    "\n",
    "Preprocess_rgb = get all rgb image data\n",
    "\n",
    "Preprocess_rgbd = get all image data infused with depth information on the blue color channel\n",
    "\n",
    "Preprocess_grayscale = get all image data that is grayscale (based on depth map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Bin Files:  71%|███████▏  | 25/35 [01:22<00:33,  3.30s/it]"
     ]
    }
   ],
   "source": [
    "# change this line from format depth to format images if needed\n",
    "train_images, train_masks, val_images, val_masks, test_images, test_masks = preprocess_grayscale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, len(train_images))\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "ax[0].imshow(train_images[idx])\n",
    "ax[1].imshow(train_masks[idx], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image_array(image_array_raw, image_array_binary, num_augmentations):\n",
    "    \n",
    "    aug_raw = []\n",
    "    aug_masks = []\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "            for i in range(len(image_array_raw) -1):\n",
    "                image_raw = image_array_raw[i]\n",
    "                image_binary = image_array_binary[i]\n",
    "\n",
    "                flipped_image_raw = cv2.flip(image_raw, 1)\n",
    "                flipped_image_binary = cv2.flip(image_binary, 1)\n",
    "\n",
    "                angle = random.uniform(-30, 30)\n",
    "                (h, w) = flipped_image_raw.shape[:2]\n",
    "                center = (w // 2, h // 2)\n",
    "\n",
    "                M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "                augmented_image_raw = cv2.warpAffine(flipped_image_raw, M, (w, h))\n",
    "                augmented_image_binary = cv2.warpAffine(flipped_image_binary, M, (w, h))\n",
    "\n",
    "                aug_raw.append(augmented_image_raw)\n",
    "                aug_masks.append(augmented_image_binary)\n",
    "\n",
    "    image_array_raw = np.concatenate((image_array_raw, np.array(aug_raw)))\n",
    "    image_array_binary = np.concatenate((image_array_binary, np.array(aug_masks)))\n",
    "\n",
    "    return image_array_raw, image_array_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# apply augmentation to arrays \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m train_images, train_masks \u001b[38;5;241m=\u001b[39m augment_image_array(train_images, train_masks, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      4\u001b[0m val_images, val_masks \u001b[38;5;241m=\u001b[39m augment_image_array(val_images, val_masks, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      5\u001b[0m test_images, test_masks \u001b[38;5;241m=\u001b[39m augment_image_array(test_images, test_masks, \u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36maugment_image_array\u001b[0;34m(image_array_raw, image_array_binary, num_augmentations)\u001b[0m\n\u001b[1;32m     16\u001b[0m center \u001b[38;5;241m=\u001b[39m (w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     18\u001b[0m M \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mgetRotationMatrix2D(center, angle, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m augmented_image_raw \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpAffine(flipped_image_raw, M, (w, h))\n\u001b[1;32m     20\u001b[0m augmented_image_binary \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpAffine(flipped_image_binary, M, (w, h))\n\u001b[1;32m     22\u001b[0m aug_raw\u001b[38;5;241m.\u001b[39mappend(augmented_image_raw)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# apply augmentation to arrays \n",
    "\n",
    "train_images, train_masks = augment_image_array(train_images, train_masks, 100)\n",
    "val_images, val_masks = augment_image_array(val_images, val_masks, 100)\n",
    "test_images, test_masks = augment_image_array(test_images, test_masks, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print new lengths \n",
    "\n",
    "print(len(train_images))\n",
    "print(len(val_images))\n",
    "print(len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "training_dataset_dict = {\n",
    "    \"image\": [Image.fromarray(img) for img in train_images],\n",
    "    \"label\": [Image.fromarray(mask) for mask in train_masks],\n",
    "}\n",
    "\n",
    "val_dataset_dict = {\n",
    "    \"image\": [Image.fromarray(img) for img in val_images],\n",
    "    \"label\": [Image.fromarray(mask) for mask in val_masks],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create the dataset using the datasets.Dataset class\n",
    "training_dataset = Dataset.from_dict(training_dataset_dict)\n",
    "val_dataset = Dataset.from_dict(val_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "  def __init__(self, dataset, processor):\n",
    "    self.dataset = dataset\n",
    "    self.processor = processor\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    image = item[\"image\"]\n",
    "    ground_truth_mask = np.array(item[\"label\"])\n",
    "\n",
    "    # get bounding box prompt\n",
    "    prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "    # prepare image and prompt for the model\n",
    "    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "    # remove batch dimension which the processor adds by default\n",
    "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "\n",
    "    # add ground truth segmentation\n",
    "    inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamProcessor\n",
    "from torch.optim import Adam\n",
    "import monai\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from transformers import SamModel\n",
    "\n",
    "# import sam processor\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# get an instance of SAMDataset using our training and val datasets\n",
    "training_dataset = SAMDataset(dataset=training_dataset, processor=processor)\n",
    "val_dataset = SAMDataset(dataset=val_dataset, processor=processor)\n",
    "\n",
    "# dataloader \n",
    "train_dataloader = DataLoader(dataset=training_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=2, shuffle=False) \n",
    "\n",
    "# get pretrained model that we are going to fine tune \n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai\n",
    "\n",
    "# Note: Hyperparameter tuning could improve performance here\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "# config \n",
    "num_epochs = 20\n",
    "folder_path_model = ''\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                        input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                        multimask_output=False)\n",
    "\n",
    "        # Compute loss\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "        # Backward pass (compute gradients)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    # Logging training results\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean training loss: {mean(epoch_losses)}')\n",
    "\n",
    "    # validation loop \n",
    "    model.eval()  \n",
    "    val_losses = []\n",
    "    with torch.no_grad():  \n",
    "        for batch in tqdm(val_dataloader):\n",
    "            outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                            input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                            multimask_output=False)\n",
    "\n",
    "           \n",
    "            predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "            loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    print(f'Mean validation loss: {mean(val_losses)}')\n",
    "    \n",
    "# save model to folder path\n",
    "torch.save(model.state_dict(), folder_path_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tumor-env [~/.conda/envs/tumor-env/]",
   "language": "python",
   "name": "conda_tumor-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
